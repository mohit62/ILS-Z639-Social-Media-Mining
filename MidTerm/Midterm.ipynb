{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is an individual assignment. Do not work in groups, and do not consult each other when doing this midterm.\n",
    "\n",
    "# Task 1 - Getting tweets (3 points)\n",
    "\n",
    "Pick either two Twitter usernames. These usernames should be \"oppositional\" in nature. I'm leaving the exact meaning of the term \"oppositional\" intentional vague; the two usernames you pick should be in the same domain (politics, products, pop culture, etc.) but should also be strong contrasts or even rivals. For example:  @Apple vs. @SamsungMobile, @SenMajLdr vs. @SenSchumer, @CocaCola vs. @pepsi, @OfficialKanye vs. @taylorswift13. Basically, if the two handles/hashtags you pick can sensibly fit into the phrase \"A vs. B,\" then they're oppositional.\n",
    "\n",
    "In the code block below, use the twitter API to get **the text** of 500 tweets representing the two usernames you pick - specifically, the 500 most recent tweets of the users. Please pick users that tweet primarily in English. This \"English\" requirement is simply so the two instructors can go through the information. This means you'll have a total of 1000 tweets.\n",
    "\n",
    "**Do NOT pick any of the examples I gave above above as your two oppositional usernames. Think up an original pair. Note that this task requires you to come up with two \"oppositional\" entities as well as to get those entities most recent tweets. Keep in mind that this is an individual assignment. Any two students who happen to pick the same two oppositional entities may be double checked for cheating, therefore, the more \"original\" your oppositional pair, the better. Any two students who have exactly the same set of tweets will draw a high amount of suspicion, since it is highly unlikely students will a) pick the same entities and b) get their tweets at exactly the same time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save twitter credentials in variables over here\n",
    "API_KEY = \"\"\n",
    "API_SECRET = \"\"\n",
    "import tweepy\n",
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "#fetching tweets for both Southwest and United airlines form their user timelines\n",
    "user1_tweets= tweepy.Cursor(api.user_timeline, id=\"SouthwestAir\")\n",
    "user2_tweets = tweepy.Cursor(api.user_timeline, id=\"united\")\n",
    "user_tweetstore = []\n",
    "for status1,status2 in zip(user1_tweets.items(500),user2_tweets.items(500)):\n",
    "    user_tweetstore.append((status1.text,0))\n",
    "    user_tweetstore.append((status2.text,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Rubric\n",
    "\n",
    "- code to obtain Tweets runs without errors (1 point)\n",
    "- code correctly written to obtain the latest 500 tweets from a pair of usernames (1 point)\n",
    "- student REMOVES their API key and secret before submitting the midterm!!! (1 point)\n",
    "\n",
    "# Task 2 - Saving your data to an external file (3 points)\n",
    "\n",
    "In the code block below, consolidate your tweets into a single variable. This variable should have two \"columns,\" one for the text of the tweets, the other a binary indicator of the source (e.g. 0 for the first source, 1 for the second). You can use a list of tuples or A Pandas dataframe, and save as either a pickle (.pkl) or a comma-seperated values file (.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Category of Airlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Morb1dlyObtuse Hey, there. Could you DM a scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@HappyYugina Hello, please DM your confirmatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kat_bates We don't like to hear of your conti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@MSIDarbs Have a great trip, David! ^EM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@erik_pederson We know delays are no fun, Erik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@DJENNINGS15 Hi there. Yes, that's correct. ^CW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Matt4Music Woohoo! That's what we love to hea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@mr_audrey Hi Kate have you since been reunite...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@itstreverr Hey, Trever. We appreciate your fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@rcrain Hello Rhiannon, let's look at your res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@PointsToPointB Inflight Live TV is a thing of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@BobbyRoby1 We understand your frustration, Bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@davidjdeal Hey, David. For more information r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@MattJLeone So happy to hear that, Matthew. We...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@BretHoovler Oh no, we're sorry to hear about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@m_vang Cheers to that! ^CW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@allhailroberto Our apologies, Robert. We limi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@vdostoi1 We understand, Vasya. Our team is wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@camschorizo Our apologies for the difficultie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@RBamber We hope you enjoyed it, Richard! ^CW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@WrightWin We know attitude is everything when...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@miladseyedi Hello Milad, we're sorry to hear ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@MichiganManInGA It's never our intent to disa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@plasticlobster Thank you for letting us know....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@wingod Our apologies for the difficulties, De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@haisondeleon Hey Hai-son. We're sorry to hear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@MichiganManInGA We never like to hear we've d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@WheelSpin_FuelS Thank you for making us aware...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@CassTrophy Aw, those cupcakes are almost as s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@JulieZucker3 Hi Julie, please send over your ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>@DOObaby2218 We're sorry for the delay, Duane....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>@draashishkshah Good morning, Aashish. Have a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>@BFlyHSam Great photos, Hilary! Thanks for sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>@secretgolf_Joel Joel, we dislike that you're ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>@anneharvey328 Hey, Anne. Our goal is an ontim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>@UMDChelseaT Hey Chelsea, how long ago was the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>@NOMADKW Was our pleasure having you both onbo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>@janztweet We're disappointed to hear we've le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>@PetePirone Very sorry about the cleanliness o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>@JimSias We're disappointed to hear this, Jim....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>@JetSetReport Oh no, we're sorry for any incon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>@jlynn601 Hello, have you spoken with a team m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>@KingRizko Hey, Mark. We don't like to hear we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>@zkerravala It's the little gestures that go a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Looking for a vacation? Join our Community by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>@kayla_macias We'e sorry if you were inconveni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>@bryanwempen Bryan, your Loyalty means the wor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>@mr_mason Please DM us your tag number, and we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>@tekfox Trvp, we certainly aim for much smooth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>@Neuggs We're sorry to read that. If you mind ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>@TheChipogriff Our apologies for any disappoin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>@notnikkogianino Let's talk about what's going...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>@MaryKayHyde We aim to please, Mary Kay! Your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>@DrayZera Congratulations on your first perfor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>@LS_MO13 Was a pleasure sharing the skies with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>@scottmc70 Scott, we're sorry you're held up. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>@tmkincaid Your love means the world to us, To...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>@davidjlatt We're happy to see you back in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>@MeeganAddy Hi, Meegan. Let's take a closer lo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>@ScaryMaryLDN Mary, we never want you to feel ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Category of Airlines\n",
       "0    @Morb1dlyObtuse Hey, there. Could you DM a scr...                     0\n",
       "1    @HappyYugina Hello, please DM your confirmatio...                     1\n",
       "2    @kat_bates We don't like to hear of your conti...                     0\n",
       "3              @MSIDarbs Have a great trip, David! ^EM                     1\n",
       "4    @erik_pederson We know delays are no fun, Erik...                     0\n",
       "5      @DJENNINGS15 Hi there. Yes, that's correct. ^CW                     1\n",
       "6    @Matt4Music Woohoo! That's what we love to hea...                     0\n",
       "7    @mr_audrey Hi Kate have you since been reunite...                     1\n",
       "8    @itstreverr Hey, Trever. We appreciate your fe...                     0\n",
       "9    @rcrain Hello Rhiannon, let's look at your res...                     1\n",
       "10   @PointsToPointB Inflight Live TV is a thing of...                     0\n",
       "11   @BobbyRoby1 We understand your frustration, Bo...                     1\n",
       "12   @davidjdeal Hey, David. For more information r...                     0\n",
       "13   @MattJLeone So happy to hear that, Matthew. We...                     1\n",
       "14   @BretHoovler Oh no, we're sorry to hear about ...                     0\n",
       "15                         @m_vang Cheers to that! ^CW                     1\n",
       "16   @allhailroberto Our apologies, Robert. We limi...                     0\n",
       "17   @vdostoi1 We understand, Vasya. Our team is wo...                     1\n",
       "18   @camschorizo Our apologies for the difficultie...                     0\n",
       "19       @RBamber We hope you enjoyed it, Richard! ^CW                     1\n",
       "20   @WrightWin We know attitude is everything when...                     0\n",
       "21   @miladseyedi Hello Milad, we're sorry to hear ...                     1\n",
       "22   @MichiganManInGA It's never our intent to disa...                     0\n",
       "23   @plasticlobster Thank you for letting us know....                     1\n",
       "24   @wingod Our apologies for the difficulties, De...                     0\n",
       "25   @haisondeleon Hey Hai-son. We're sorry to hear...                     1\n",
       "26   @MichiganManInGA We never like to hear we've d...                     0\n",
       "27   @WheelSpin_FuelS Thank you for making us aware...                     1\n",
       "28   @CassTrophy Aw, those cupcakes are almost as s...                     0\n",
       "29   @JulieZucker3 Hi Julie, please send over your ...                     1\n",
       "..                                                 ...                   ...\n",
       "970  @DOObaby2218 We're sorry for the delay, Duane....                     0\n",
       "971  @draashishkshah Good morning, Aashish. Have a ...                     1\n",
       "972  @BFlyHSam Great photos, Hilary! Thanks for sha...                     0\n",
       "973  @secretgolf_Joel Joel, we dislike that you're ...                     1\n",
       "974  @anneharvey328 Hey, Anne. Our goal is an ontim...                     0\n",
       "975  @UMDChelseaT Hey Chelsea, how long ago was the...                     1\n",
       "976  @NOMADKW Was our pleasure having you both onbo...                     0\n",
       "977  @janztweet We're disappointed to hear we've le...                     1\n",
       "978  @PetePirone Very sorry about the cleanliness o...                     0\n",
       "979  @JimSias We're disappointed to hear this, Jim....                     1\n",
       "980  @JetSetReport Oh no, we're sorry for any incon...                     0\n",
       "981  @jlynn601 Hello, have you spoken with a team m...                     1\n",
       "982  @KingRizko Hey, Mark. We don't like to hear we...                     0\n",
       "983  @zkerravala It's the little gestures that go a...                     1\n",
       "984  Looking for a vacation? Join our Community by ...                     0\n",
       "985  @kayla_macias We'e sorry if you were inconveni...                     1\n",
       "986  @bryanwempen Bryan, your Loyalty means the wor...                     0\n",
       "987  @mr_mason Please DM us your tag number, and we...                     1\n",
       "988  @tekfox Trvp, we certainly aim for much smooth...                     0\n",
       "989  @Neuggs We're sorry to read that. If you mind ...                     1\n",
       "990  @TheChipogriff Our apologies for any disappoin...                     0\n",
       "991  @notnikkogianino Let's talk about what's going...                     1\n",
       "992  @MaryKayHyde We aim to please, Mary Kay! Your ...                     0\n",
       "993  @DrayZera Congratulations on your first perfor...                     1\n",
       "994  @LS_MO13 Was a pleasure sharing the skies with...                     0\n",
       "995  @scottmc70 Scott, we're sorry you're held up. ...                     1\n",
       "996  @tmkincaid Your love means the world to us, To...                     0\n",
       "997  @davidjlatt We're happy to see you back in the...                     1\n",
       "998  @MeeganAddy Hi, Meegan. Let's take a closer lo...                     0\n",
       "999  @ScaryMaryLDN Mary, we never want you to feel ...                     1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "#Converting tweets from both the airlines into a dataframe with 0 for SouthwestAir and 1 for united.\n",
    "Airline_Tweets = pd.DataFrame(user_tweetstore,columns=[\"Tweet\",\"Category of Airlines\"])\n",
    "pkl.dump(Airline_Tweets, open(\"AirlineTweets.pkl\", \"wb\"))\n",
    "pkl.load(open(\"AirlineTweets.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Rubric\n",
    "\n",
    "- code to save data to external file runs without errors (1 point)\n",
    "- saved data formatted correctly into \"2 columns\" (1 point)\n",
    "- external file submitted with midterm (1 point)\n",
    "\n",
    "# Task 3 - Preparing data for an sklearn binary classifier (3 points)\n",
    "\n",
    "In the code block below, create two variables, X, and y. The y variable should be simple - it is simply the \"second column\" of the data you made in task 2, a binary indicator of source, with 0 representing one source and 1 representing the other.\n",
    "\n",
    "For the X variable, choose either `TfidifVectorizer` or `CountVectorizer` from `sklearn.feature_representation.text` to turn the raw text (column 1 from task 2) into a \"bag-of-words\" representation. When instantiating your vectorizer, set the argument `lowercase=True`, to ensure that all words are lowercased, and `stopwords=\"english\"`, to remove English stop words. \n",
    "\n",
    "Additional, when instantiating the vectorizer, pass the `max_df=???` and `min_df=???` arguments. These arguments can either take a *float between 0.0 and 1.0 or an integer*. The df stands for \"document frequency.\" These arguments tell the vectorizer to remove words that occur *over* (max_df) and *under* (min_df) a certain amount of documents. This will remove frequent words - which show up all the time and therefore are not informative - and infrequent words, which are so rare as to just be noise. If you pass these arguments a float, that float represents the proportion of documents (e.g. `max_df=0.9` means, remove all words that show up in more than 90% of the tweets) and if you pass these arguments an integer, that integer represents the number of documents (e.g. `min_df=5`, remove all words that show up in less than 5 document). \n",
    "\n",
    "By the end of this task, you should have a variable X, of dimensionality $n \\times d$ where $n = 1000$ and $d$ is the number of words left after the vectorizer considers df, and you should have variable y, which is a vector of length 1000, with 1s or 0s representing tweet source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(stop_words='english',lowercase=True,max_df=0.8,min_df=3)\n",
    "X = tv.fit_transform(Airline_Tweets[\"Tweet\"])\n",
    "y=Airline_Tweets[\"Category of Airlines\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Rubric\n",
    "\n",
    "- Code runs without errors (1 point)\n",
    "- Written code correctly achieves objective of creating X, y variables for classifier (1 point)\n",
    "- All required arguments to vectorizer included (1 point)\n",
    "\n",
    "\n",
    "# Task 4 - Training a Logistic Regression classifier (3 points)\n",
    "\n",
    "Instantiate an sklearn Logistic Regression binary classifier ([sklearn documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). \n",
    "\n",
    "Then, use `cross_val_score` from `sklearn.model_selection` ([sklearn documentation here](http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics))to perform 5-fold cross validation. The inputs to `cross_val_score` will be your instantiated Logistic Regression classifier, X, y, and a named argument `cv=5` to indicate the number of folds. The output will be a list of 5 numbers - the accuracy from each fold.\n",
    "\n",
    "Print the average of those 5 numbers. This will be the mean 5-fold cross validation accuracy of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#using logistic regression\n",
    "clf = LogisticRegression()\n",
    "#performing crossvalidation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Rubric\n",
    "\n",
    "- Code runs without errors (1 point)\n",
    "- Code successfully creates a Logistic Regression classifier and runs cross validation (1 point)\n",
    "- Code prints mean cross validation accuracy (1 point)\n",
    "\n",
    "# Task 5 - Discussion\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "1 Since you pulled equal amounts of tweets from each source, the baseline accuracy is 50%. This is the accuracy we would expect from a classifier that guessed 0 or 1 randomly, or a classifier that simply guessed all 0s or all 1s. Your classifier either did well or did poorly. In either case, think about the *actual content* of the sources you picked and in the text block below, informally share your thoughts on why your classifier did poorly/well. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The classifier performed well because we removed noise from the data like the stopwords and the most common and the least common words thus including only the relevant features which contribute significantly in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 What could you have done differently when preprocessing your data (task 3) to try and improve your classifier's accuracy? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. We can fine tune the df_max and df_min parameters while preprocessing the data to get better classifier accuracy.We can also check stopwords for other languages to further remove the noise from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 What parameters could you have adjusted in the Logistic Regression classifier in Task 4 to \"tune\" it and get better performance? What other binary classifiers could you have used, and what \"tune-able\" parameters do those classifiers have? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. I think one potential way to further improve the accuracy of the model can be tuning the parameter C which is a positive number inversely proportional to the regularization strength i.e. lower the C stronger the regularization[1].\n",
    "We could have used Support Vector Machine which has tunable parameters like C i.e. the penalty parameter of the error term and kernel type which is 'rbf' by default but can be tuned to other types as well[2].\n",
    "# Reference:\n",
    "[1] http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "[2] http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus (3 points)\n",
    "\n",
    "This is an opportunity to gain an additional 3 points above the 15 allocated for the midterm. This will require you to do something that wasn't covered directly in the lectures, but can somewhat easily be learned by going through the sklearn documentation.\n",
    "\n",
    "Note 1: When a count vectorizer or TF-IDF vectorizer is instantiated and used to transform your raw text data, it builds a dictionary that indicates which word is assigned to which index. Remember that it produces an $n \\times d$ matrix, where $n$ is the number of samples and $d$ is the number of words. If you want to know the index of a word (that is, which column in $d$ corresponds to that word), you can consult this dictionary. Suppose you named your vectorizer `vec`. To access this dictionary, use `vec.vocabulary_`. If you want to know the index of the word `banana`, access `vec.vocabulary_['banana']`. \n",
    "\n",
    "Note 2: When you instantiate and train a logistic regression, it saves a set of *coefficients* indicating the \"weight\" of that word in terms of predicting the outcome variable. Suppose you named your classifier `lr`. You can access these coefficents at `lr.coef_[1]`. (the `[1]` is there because `lr.coef_[0]` is where the intercept of the model is stored). This means that `lr.coef_[1][0]` is the weight of the 0th feature, `lr.coef_[1][1]` is the weight of the 1st feature, and so on.\n",
    "\n",
    "You can therefore *iterate* through `vec.vocabulary_.items()`, and for each word (key) get its index (value) and then find the coefficient weight of that word in the model `lr.coef_[1][index]`. \n",
    "\n",
    "In the code block (or blocks, if you want to make more than one to organize your code better) below:\n",
    "\n",
    "1. Instantiate a *new* instance of a Logistic Regression classifier, `fit` that classifier on X and y. (1 point)\n",
    "2. Use the notes above to make a list of tuples, where the first value in each tuple is a *word in the vocabulary* and the second value is the *coefficient weight assigned to that word in the trained Logistic Regression classifier*. Sort that list of tuples by the second value (the weight) ([Here's how you can do that](https://stackoverflow.com/questions/10695139/sort-a-list-of-tuples-by-2nd-item-integer-value)). (1 point)\n",
    "3. Print the 10 words with the highest weights and the 10 words with the lowest weights. In a few sentences discuss whether these words help you understand why the model performed well/poorly. (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10 words with the lowest weights [('ms', -3.0556917730218784), ('ct', -2.4251110969922958), ('kd', -2.2529626546694348), ('vp', -2.1753336848987805), ('lj', -1.8893903382038322), ('onboard', -1.8725958264992673), ('ac', -1.8230466163576804), ('jt', -1.5340248554216607), ('mk', -1.4181223008180242), ('nc', -1.3145787759875553)] \n",
      "\n",
      "10 words with the highest weights [('dp', 1.8830109665928776), ('kf', 2.0170989026563229), ('y6hg6uklar', 2.2503818951031502), ('kl', 2.2736288919025731), ('em', 2.2762496643445829), ('sv', 2.3131771130156382), ('eb', 2.4362855978806643), ('cw', 2.5773109385964945), ('md', 2.6712757552519282), ('ad', 3.8400422563389491)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3057)\n",
    "#Instantiated Logistic Regression classifier to fit on X and y\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "weight=[]\n",
    "#get words alongwith their weights \n",
    "for keys,value in tv.vocabulary_.items():\n",
    "    weight.append((keys,lr.coef_[0][value]))\n",
    "#sorted words on th basis of their weights    \n",
    "sortedweights=sorted(weight, key=lambda weight: weight[1])\n",
    "#print the 10 words with the lowest weights\n",
    "print (\"the 10 words with the lowest weights\",sortedweights[0:10],\"\\n\")\n",
    "#print the 10 words with the highest weights\n",
    "print (\"10 words with the highest weights\",sortedweights[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words with the highest weights signify that these are the most important features in our model whereas the words with the lowest weights signify that these are the least important features in our model and thus more important features contribute more significantly to the outcome of model than the least important one and thus the model did well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
